{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd813fc",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7daf2e9",
   "metadata": {},
   "source": [
    "The goal of this notebook is to introduce some basic concepts of Machine Learning within the context of (computational chemistry). We will focus on a specific dataset from Therapeutic Data Commons (TDC; https://tdcommons.ai), and we will make use of RDKit to featurize our data points and scikit-learn to design our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33972e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #data manipulation\n",
    "from tqdm.auto import tqdm #progress bar\n",
    "from rdkit import Chem #Chemistry\n",
    "from rdkit.Chem import rdMolDescriptors #molecular descriptors\n",
    "import numpy as np #matrix algebra\n",
    "from sklearn.model_selection import train_test_split #ML training\n",
    "from sklearn.metrics import r2_score, mean_squared_error #ML stats\n",
    "import seaborn as sns #Plotting\n",
    "from tdc.single_pred import ADME\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import rdBase\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "rdBase.DisableLog('rdApp.*')\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b4b13",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d168b76",
   "metadata": {},
   "source": [
    "A good starting point of any machine learning study is to start by analyzing your dataset. Here, we will load a fully curated dataset from TDC, hence no particular data cleaning steps (e.g., filtering out incorrect data points) will be needed -- note that this is usually essential when you generate a dataset from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ADME(name = 'Solubility_AqSolDB')\n",
    "sol_df = data.get_data()\n",
    "sol_df.columns = [\"Name\",\"SMILES\",\"LogS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728904cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sol_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2924d49",
   "metadata": {},
   "source": [
    "Since the dataset has already been cleaned before, we will only generate a distribution of the labels, i.e., the LogS values -- as you can see in the histogram below, there is only one slight outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the histogram\n",
    "sns.histplot(sol_df['LogS'], kde=False)\n",
    "\n",
    "# Customize the plot (optional)\n",
    "plt.title('Distribution of LogS values')\n",
    "plt.xlabel('LogS')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca767239",
   "metadata": {},
   "source": [
    "### Featurization -- Fingerprints vs. descriptor representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d601ef3",
   "metadata": {},
   "source": [
    "In the loaded dataset, the datapoints are represented by SMILES strings. If we want to build a machine learning model, we need to generate a mathematical/vectorized description of the datapoint, i.e., we need to generate a representation/featurization.\n",
    "\n",
    "Several strategies can be taken to featurize molecules based on SMILES input. We will first focus on molecular fingerprints (if you have no idea what fingerprints are, you can take a look here: https://open-babel.readthedocs.io/en/latest/Fingerprints/intro.html#:~:text=Molecular%20fingerprints%20are%20a%20way,particular%20substructures%20in%20the%20molecule.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a92371",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "import rdkit\n",
    "\n",
    "mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2,fpSize=2048)\n",
    "\n",
    "def get_fingerprint(smiles, generator=mfpgen):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return np.array(generator.GetFingerprint(mol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6809ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_df['fingerprint'] = sol_df['SMILES'].progress_apply(lambda x: get_fingerprint(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8a0df",
   "metadata": {},
   "source": [
    "Let's build a first simplistic machine learning model with this featurization, a random forest (don't pay too much attention to the parameters used to set up the model for now). We start by splitting the data in a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(sol_df, random_state=42)\n",
    "\n",
    "train_X = np.vstack(train['fingerprint'])\n",
    "train_y = train.LogS\n",
    "test_X = np.vstack(test['fingerprint'])\n",
    "test_y = test.LogS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080142a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_depth=10, n_estimators=100, max_features=0.2, random_state=42)\n",
    "# train the model\n",
    "rf.fit(train_X, train_y)\n",
    "# predict\n",
    "pred = rf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=test_y,y=pred,scatter_kws={'s':10})\n",
    "ax.set(xlabel=\"Experimental LogS\")\n",
    "ax.set(ylabel=\"Predicted LogS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f70d0",
   "metadata": {},
   "source": [
    "Let's calculate some metrics for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(test_y,pred))\n",
    "print(mean_squared_error(test_y,pred,squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac3f0e",
   "metadata": {},
   "source": [
    "Now, we will turn to a common alternative representation of fingerprints, one that is not based on a one-hot encoding of substructure patterns. Often, molecules (or reactions) are represented with the help of computed descriptors. Computed descriptors are typically determined with the help of electronic structure methods, e.g.,  wavefunction approaches, DFT or xTB (e.g., partial charges, orbital coefficients, steric descriptors). Because of the size of our dataset, as well as time- and resource-constraints, we will make use of a simpler method to compute descriptors, i.e., we will extract the built-in descriptors from RDKit, which are computed rapidly since they don't require (self-consistently) solving the Schr√∂dinger equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by extracting a list of available descriptors from rdkit and we define a function to compute them all at once\n",
    "property_names = list(rdMolDescriptors.Properties.GetAvailableProperties())\n",
    "property_getter = rdMolDescriptors.Properties(property_names)\n",
    "\n",
    "def smi2props(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    props = None\n",
    "    if mol:\n",
    "        #Chem.DeleteSubstructs(mol, Chem.MolFromSmarts(\"[#1X0]\"))\n",
    "        props = np.array(property_getter.ComputeProperties(mol))\n",
    "    return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdccf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_df['props'] = sol_df.SMILES.progress_apply(smi2props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f311ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_df[property_names] = sol_df['props'].to_list()\n",
    "train, test = train_test_split(sol_df, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[property_names]\n",
    "train_y = train.LogS\n",
    "test_X = test[property_names]\n",
    "test_y = test.LogS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac5e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestRegressor(max_depth=10, n_estimators=100, max_features=0.2, random_state=42)\n",
    "# train the model\n",
    "rf2.fit(train_X, train_y)\n",
    "# predict\n",
    "pred = rf2.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=test_y,y=pred,scatter_kws={'s':10})\n",
    "ax.set(xlabel=\"Experimental LogS\")\n",
    "ax.set(ylabel=\"Predicted LogS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(test_y,pred))\n",
    "print(mean_squared_error(test_y,pred,squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31821c8d",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dce3e7",
   "metadata": {},
   "source": [
    "So, it appear that on the tested train-test split, the constructed descriptor model outperforms by far the fingerprint model, but is this always the case? Are we certain that changing the make-up of train and test set doesn't change this conclusion? \n",
    "\n",
    "These questions bring us to the broader question of how we compare model performances. A robust approach to compare models is cross-validation (https://scikit-learn.org/stable/modules/cross_validation.html), where the dataset is split many different times, so that every data-point ends up in the test set once. Below, we will perform a cross-validation for both model architectures introduced so far (this can take a couple of minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X = np.vstack(sol_df['fingerprint'])\n",
    "y = sol_df.LogS\n",
    "\n",
    "cv_results = cross_validate(rf, X, y, cv=5, scoring=('r2', 'neg_mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fbb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_results['test_r2'])\n",
    "rmse_list = [np.sqrt(-cv_result) for cv_result in cv_results['test_neg_mean_squared_error']]\n",
    "print(rmse_list)\n",
    "\n",
    "mean_rmse_across_folds = sum(rmse_list) / len(cv_results['test_neg_mean_squared_error'])\n",
    "print(f'The mean RMSE across the folds is: {mean_rmse_across_folds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb3e89",
   "metadata": {},
   "source": [
    "The accuracy of our model appears very split-dependent, and the result we obtained for the original split appears to have been unrealistically optimistic. We now do the same for the descriptor-representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.vstack(sol_df['props'])\n",
    "y = sol_df.LogS\n",
    "\n",
    "cv_results_desc = cross_validate(rf, X2, y, cv=5, scoring=('r2', 'neg_mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_results_desc['test_r2'])\n",
    "rmse_list_desc = [np.sqrt(-cv_result) for cv_result in cv_results_desc['test_neg_mean_squared_error']]\n",
    "print(rmse_list_desc)\n",
    "\n",
    "mean_rmse_across_folds_desc = sum(rmse_list_desc) / len(cv_results_desc['test_neg_mean_squared_error'])\n",
    "print(f'The mean RMSE across the folds is: {mean_rmse_across_folds_desc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ff38f",
   "metadata": {},
   "source": [
    "Note that the accuracy varies significantly across folds for both model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232cb28c",
   "metadata": {},
   "source": [
    "Overall, the fingerprint model appears to work significantly better. As such, we will focus on this representation in the remainder of this Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bdeeb6",
   "metadata": {},
   "source": [
    "What we still haven't touched upon is the selection of the parameters during the setup of our Random Forest (see above). These parameters are called the hyperparameters of the model, and they greatly affect the performance that the trained model can reach. Fine-tuning them directly to minimize the test error is not a good idea, since this creates a risk for model overfitting. To avoid this, we typically determine the hyperparameters exclusively based on the train-set. This ensures no data leakage, which results in overfitting, takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaf446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "train, test = train_test_split(sol_df, random_state=42)\n",
    "train_X = np.vstack(train['props'])\n",
    "train_y = train.LogS\n",
    "test_X = np.vstack(test['props'])\n",
    "test_y = test.LogS\n",
    "\n",
    "hyper_cv = KFold(n_splits=4, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff394f",
   "metadata": {},
   "source": [
    "Now we will run a grid search of various combinations of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8fdcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up possible values of the hyperparameters to optimize over\n",
    "p_grid = {\"max_depth\": [25, 50, 75], \"n_estimators\": [100, 200, 300], \"max_features\": [0.2, 0.5, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf3 = RandomForestRegressor(random_state=42)\n",
    "hyper_search = GridSearchCV(estimator=rf3, param_grid=p_grid, cv=hyper_cv)\n",
    "hyper_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8336264",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyper_search.best_score_)\n",
    "print(hyper_search.best_estimator_)\n",
    "print(hyper_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89c7ff",
   "metadata": {},
   "source": [
    "We set the hyperparameters to the optimal ones emerging from our grid search. To get a sense about how well our model will perform on unseen data points, we can apply our model to the (unseen) test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada4ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_final = RandomForestRegressor(max_depth=hyper_search.best_params_['max_depth'], max_features=hyper_search.best_params_['max_features'], \n",
    "                                 n_estimators=hyper_search.best_params_['n_estimators'], random_state=42)\n",
    "rf_final.fit(train_X, train_y)\n",
    "\n",
    "pred = rf_final.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=test_y,y=pred,scatter_kws={'s':10})\n",
    "ax.set(xlabel=\"Experimental LogS\")\n",
    "ax.set(ylabel=\"Predicted LogS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(test_y,pred))\n",
    "print(mean_squared_error(test_y,pred,squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278e260",
   "metadata": {},
   "source": [
    "So, we obtain a slight improvement over the performance with the original, non-fine-tuned, hyperparameters. It should be noted that once again, we only evaluated 1 split here; in practice we will often perform a so-called \"nested cross-validation\" to evaluate and compare the performance of different model architectures across hyperparameter settings (cf. https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
